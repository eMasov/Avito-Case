{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13138758,"sourceType":"datasetVersion","datasetId":8323950}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport json\nimport random\nimport csv\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import IterableDataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset\nfrom functools import lru_cache\nimport math\nimport pandas as pd\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-24T19:43:51.401089Z","iopub.execute_input":"2025-09-24T19:43:51.401259Z","iopub.status.idle":"2025-09-24T19:44:04.185425Z","shell.execute_reply.started":"2025-09-24T19:43:51.401242Z","shell.execute_reply":"2025-09-24T19:44:04.184789Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75433cf74bf84e498b5f399f0d15028f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad73d53e731f43f29f06b4154d61195f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c03677348084c4c89ee0377a0051dcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9208047b82c4a849b01374789c87be7"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"data = []\nwith open('/kaggle/input/dataset-1937770-3-txt/dataset_1937770_3.txt', 'r', encoding='utf-8') as f:\n\n    header = f.readline().strip().split(',')\n\n    for line in f:\n        line = line.strip()\n\n        first_comma_index = line.find(',')\n\n        if first_comma_index != -1:\n\n            row_id = line[:first_comma_index]\n            text = line[first_comma_index + 1:]\n            data.append([row_id, text])\n\n\ntask_data = pd.DataFrame(data, columns=['id', 'text_no_spaces'])\ntask_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T19:44:04.259216Z","iopub.execute_input":"2025-09-24T19:44:04.259433Z","iopub.status.idle":"2025-09-24T19:44:04.291540Z","shell.execute_reply.started":"2025-09-24T19:44:04.259415Z","shell.execute_reply":"2025-09-24T19:44:04.290926Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  id                 text_no_spaces\n0  0                куплюайфон14про\n1  1             ищудомвПодмосковье\n2  2  сдаюквартирусмебельюитехникой\n3  3     новыйдивандоставканедорого\n4  4                отдамдаромкошку","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text_no_spaces</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>куплюайфон14про</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ищудомвПодмосковье</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>сдаюквартирусмебельюитехникой</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>новыйдивандоставканедорого</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>отдамдаромкошку</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Эвристики для разбиения слов, обработка пунктуации, предлогов и местоимений из одной буквы. Тут же score_word (с кэшированием) с токенизатором от берта. Score_word оценивает стоимость потенциального слова при разбиении токенизатором или, если не получается, просто функцией от длины слова.","metadata":{}},{"cell_type":"code","source":"_PUNCT = set(\",.;:!?…()[]{}\\\"'—–-\")\n\ndef _is_cyr(c: str) -> bool:\n    c = c.lower()\n    return ('а' <= c <= 'я') or (c == 'ё')\n\ndef _hard_boundary(prev_ch, ch) -> bool:\n    if prev_ch is None or ch is None:\n        return False\n    if ch in _PUNCT or prev_ch in _PUNCT:\n        return True\n    if prev_ch.isdigit() != ch.isdigit():\n        return True\n    if (_is_cyr(prev_ch) != _is_cyr(ch)) and (not prev_ch.isdigit() and not ch.isdigit()):\n        return True\n    if prev_ch.islower() and ch.isupper():\n        return True\n    return False\n\n\ndef score_word(word):\n    if not word:\n        return -1e9\n    tok = tokenizer \n    toks = tok.tokenize(word)\n    if getattr(tok, \"unk_token\", None) in toks:\n        return -50.0\n    L = len(word)\n    return -abs(6 - L) * 0.1\n\n\n@lru_cache(maxsize=200_000)\ndef _cached_score_word(w):\n    return score_word(w)\n\n_SHORT_OK = {\"в\", \"к\", \"с\", \"и\", \"у\", \"я\", \"о\", \"а\"}\n\n@lru_cache(maxsize=200_000)\ndef _cached_score_word(w: str) -> float:\n    return score_word(w)\n\n_sent_re = re.compile(r\"[.!?]+\")\ndef sentences_from_text(text: str) -> List[str]:\n    text = text.replace(\"\\n\", \" \").strip()\n    sents = _sent_re.split(text)\n    return [s.strip() for s in sents if len(s.strip()) > 5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T19:44:04.292186Z","iopub.execute_input":"2025-09-24T19:44:04.292357Z","iopub.status.idle":"2025-09-24T19:44:04.299862Z","shell.execute_reply.started":"2025-09-24T19:44:04.292342Z","shell.execute_reply":"2025-09-24T19:44:04.299322Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Подгружаем RuBQ-2.0 - датасет с поисковыми запросами с hugging face для обучения LSTM, составляем словарь символ-индекс для запросов.","metadata":{}},{"cell_type":"code","source":"def stream_sources() -> Iterable[Tuple[str, str]]:\n    rubq_dev = load_dataset(\"d0rj/RuBQ_2.0\", split=\"dev\", streaming=True)\n    rubq_test = load_dataset(\"d0rj/RuBQ_2.0\", split=\"test\", streaming=True)\n\n    def head_rows(ds, limit, fields=(\"question_text\", \"answer_text\")):\n        taken = 0\n        for row in ds:\n            for f in fields:\n                txt = row.get(f)\n                if isinstance(txt, str) and txt.strip():\n                    yield txt\n                    taken += 1\n                    if taken >= limit:\n                        return\n\n    for txt in head_rows(rubq_dev, MAX_DOCS_RUBQ_QA):\n        yield (\"rubq\", txt)\n    for txt in head_rows(rubq_test, MAX_DOCS_RUBQ_QA):\n        yield (\"rubq\", txt)\n\n    if MAX_DOCS_PARAGRAPHS > 0:\n        rubq_par = load_dataset(\"d0rj/RuBQ_2.0-paragraphs\", split=\"paragraphs\", streaming=True)\n        taken = 0\n        for row in rubq_par:\n            txt = row.get(\"paragraph\") or row.get(\"paragraphs\") or row.get(\"text\")\n            if isinstance(txt, str) and txt.strip():\n                yield (\"rubq_par\", txt)\n                taken += 1\n                if taken >= MAX_DOCS_PARAGRAPHS:\n                    break\n\ndef build_alphabet() -> Tuple[dict, dict]:\n    counter = Counter()\n    seen = 0\n    for _, raw in stream_sources():\n        for s in sentences_from_text(raw):\n            s = s.lower().replace(\"ё\", \"е\")\n            counter.update(ch for ch in s if ch != \" \")\n            seen += 1\n            if seen >= WARMUP_SENT:\n                break\n        if seen >= WARMUP_SENT:\n            break\n    char2id = {\"<pad>\": 0, \"<unk>\": 1}\n    for i, ch in enumerate(sorted(counter), start=2):\n        char2id[ch] = i\n    id2char = {i: c for c, i in char2id.items()}\n    print(f\"[alphabet] unique chars: {len(char2id)} (from {seen} sentences)\")\n    return char2id, id2char\n\nchar2id, id2char = build_alphabet()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T19:44:04.300650Z","iopub.execute_input":"2025-09-24T19:44:04.301484Z","iopub.status.idle":"2025-09-24T19:44:15.235407Z","shell.execute_reply.started":"2025-09-24T19:44:04.301455Z","shell.execute_reply":"2025-09-24T19:44:15.234612Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84ec7c1582b4a80b619a59327296472"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/911 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28e8d0b88b514a938074a8bd77dd7fad"}},"metadata":{}},{"name":"stdout","text":"[alphabet] unique chars: 1017 (from 80000 sentences)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"SEED = 37\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)\n\nMAX_DOCS_RUBQ_QA = 10_000\nMAX_DOCS_PARAGRAPHS = 60_000\nMAX_SENT_TOTAL = 1_000_000\nWARMUP_SENT = 80_000\nEPOCHS = 12\nEMB_DIM = 128\nHIDDEN = 256\n\nBATCH_SIZE = 128\nMAX_LEN = 128\nLR = 1e-3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"make_xy для разметки данных (позиция - наличие пробела (1/0));\n\nитерируемый датасет с батчингом загружаем в DataLoader","metadata":{}},{"cell_type":"code","source":"def make_xy(sentence: str, char2id: dict, max_len: int):\n    s = sentence.lower().replace(\"ё\", \"е\")\n    xs, ys = [], []\n    prev_nonspace = False\n    for ch in s:\n        if ch == \" \":\n            if prev_nonspace and ys:\n                ys[-1] = 1\n            continue\n        xs.append(char2id.get(ch, 1))\n        ys.append(0)\n        prev_nonspace = True\n    if len(xs) <= 1:\n        return None\n    if len(xs) > max_len:\n        xs = xs[:max_len]\n        ys = ys[:max_len]\n    ys = ys[:-1]\n    if not ys:\n        return None\n    return torch.tensor(xs, dtype=torch.long), torch.tensor(ys, dtype=torch.float32)\n\nclass SegmIterable(IterableDataset):\n    def __init__(self, char2id, max_len, max_sent_total, split_ratio, role):\n        super().__init__()\n        self.char2id = char2id\n        self.max_len = max_len\n        self.max_sent_total = max_sent_total\n        self.split_ratio = split_ratio\n        self.role = role\n\n    def __iter__(self):\n        rng = random.Random(SEED)\n        sent_count = 0\n        for _, raw in stream_sources():\n            sents = sentences_from_text(raw)\n            rng.shuffle(sents)\n            for s in sents:\n                h = hash(s) ^ SEED\n                in_train = (h % 1000) < int(self.split_ratio * 1000)\n                if (self.role == \"train\" and not in_train) or (self.role == \"val\" and in_train):\n                    continue\n                ex = make_xy(s, self.char2id, self.max_len)\n                if ex is None:\n                    continue\n                yield ex\n                sent_count += 1\n                if sent_count >= self.max_sent_total:\n                    return\n\ndef collate_pad(batch):\n    xs, ys = zip(*batch)\n    T = max(len(x) for x in xs)\n    B = len(xs)\n    x_pad = torch.zeros((B, T), dtype=torch.long)\n    y_pad = torch.full((B, T - 1), -100.0, dtype=torch.float32)\n    lengths = []\n    for i, (x, y) in enumerate(zip(xs, ys)):\n        x_pad[i, :len(x)] = x\n        y_pad[i, :len(y)] = y\n        lengths.append(len(x))\n    return x_pad, y_pad, torch.tensor(lengths, dtype=torch.long)\n\ntrain_set = SegmIterable(char2id, MAX_LEN, int(MAX_SENT_TOTAL * 0.9), split_ratio=0.9, role=\"train\")\nval_set = SegmIterable(char2id, MAX_LEN, int(MAX_SENT_TOTAL * 0.1), split_ratio=0.9, role=\"val\")\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_pad)\nval_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_pad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T19:44:15.236499Z","iopub.execute_input":"2025-09-24T19:44:15.236822Z","iopub.status.idle":"2025-09-24T19:44:15.249272Z","shell.execute_reply.started":"2025-09-24T19:44:15.236789Z","shell.execute_reply":"2025-09-24T19:44:15.248521Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Модель -- двунаправленная LSTM: \n\nпринимает индексы символов, возвращает логиты вероятности пробела для каждой позиции.","metadata":{}},{"cell_type":"code","source":"class CharBoundaryTagger(nn.Module):\n    def __init__(self, vocab_size, emb_dim=EMB_DIM, hidden=HIDDEN, num_layers=1, dropout=0.0):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.rnn = nn.LSTM(emb_dim, hidden, num_layers=num_layers, dropout=dropout, batch_first=True, bidirectional=True)\n        self.out = nn.Linear(hidden * 2, 1)\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        h, _ = self.rnn(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        logits = self.out(h).squeeze(-1)\n        return logits[:, :-1]\n\nmodel = CharBoundaryTagger(len(char2id), num_layers=2, dropout=0.2).to(DEVICE) # <-- 2 слоя, dropout\nopt = torch.optim.Adam(model.parameters(), lr=LR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T19:44:15.249974Z","iopub.execute_input":"2025-09-24T19:44:15.250246Z","iopub.status.idle":"2025-09-24T19:44:20.018310Z","shell.execute_reply.started":"2025-09-24T19:44:15.250218Z","shell.execute_reply":"2025-09-24T19:44:20.017732Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Обучаем модель с использованием Adam'а, оцениваем бинарной кросс-энтропией. Валидация с помощью F1_score (см. функция evaluate)","metadata":{}},{"cell_type":"code","source":"def evaluate(model, loader):\n    model.eval()\n    all_true, all_pred = [], []\n    with torch.no_grad():\n        for x, y, lengths in loader:\n            x, y, lengths = x.to(DEVICE), y.to(DEVICE), lengths.to(DEVICE)\n            logits = model(x, lengths)\n            mask = (y != -100.0)\n            probs = torch.sigmoid(logits)\n            preds = (probs > 0.5).float()\n            all_true.append(y[mask].cpu())\n            all_pred.append(preds[mask].cpu())\n    if not all_true:\n        return 0.0\n    y_true = torch.cat(all_true).numpy()\n    y_pred = torch.cat(all_pred).numpy()\n    return f1_score(y_true, y_pred)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n    for x, y, lengths in pbar:\n        x, y, lengths = x.to(DEVICE), y.to(DEVICE), lengths.to(DEVICE)\n        logits = model(x, lengths)\n        mask = (y != -100.0)\n        loss = F.binary_cross_entropy_with_logits(logits[mask], y[mask])\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        pbar.set_postfix(loss=float(loss))\n    f1 = evaluate(model, val_loader)\n    print(f\"[epoch {epoch}] val F1 = {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T19:44:20.018947Z","iopub.execute_input":"2025-09-24T19:44:20.019318Z","iopub.status.idle":"2025-09-24T20:22:44.487941Z","shell.execute_reply.started":"2025-09-24T19:44:20.019300Z","shell.execute_reply":"2025-09-24T20:22:44.487089Z"}},"outputs":[{"name":"stderr","text":"Epoch 0: 1574it [02:49,  9.31it/s, loss=0.0109]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 0] val F1 = 0.9653\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 1574it [02:48,  9.34it/s, loss=0.00454]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 1] val F1 = 0.9760\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 1574it [02:47,  9.40it/s, loss=0.00145]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 2] val F1 = 0.9795\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 1574it [02:48,  9.34it/s, loss=0.00329]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 3] val F1 = 0.9816\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 1574it [02:49,  9.29it/s, loss=0.000541]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 4] val F1 = 0.9827\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 1574it [02:48,  9.32it/s, loss=0.000579]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 5] val F1 = 0.9837\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 1574it [02:52,  9.11it/s, loss=0.000566]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 6] val F1 = 0.9840\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 1574it [02:51,  9.19it/s, loss=0.00058]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 7] val F1 = 0.9844\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 1574it [02:51,  9.19it/s, loss=0.001]  \n","output_type":"stream"},{"name":"stdout","text":"[epoch 8] val F1 = 0.9844\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 1574it [02:51,  9.19it/s, loss=0.00229]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 9] val F1 = 0.9848\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 1574it [02:51,  9.19it/s, loss=7.63e-5]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 10] val F1 = 0.9852\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 1574it [02:50,  9.22it/s, loss=0.000151]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 11] val F1 = 0.9853\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Сохраняем веса модели и словарь","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"bilstm_ru_stream_v2.pth\")\nwith open(\"char2id_stream_v2.json\", \"w\", encoding=\"utf8\") as f:\n    json.dump(char2id, f, ensure_ascii=False)\nprint(\"Saved: LTSM_last.pth, char2id_stream_v2.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T20:22:44.490216Z","iopub.execute_input":"2025-09-24T20:22:44.490456Z","iopub.status.idle":"2025-09-24T20:22:44.514909Z","shell.execute_reply.started":"2025-09-24T20:22:44.490436Z","shell.execute_reply":"2025-09-24T20:22:44.514281Z"}},"outputs":[{"name":"stdout","text":"Saved: LTSM_last.pth, char2id_stream_v2.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Дальше восстанавливаем модель и инференсим","metadata":{}},{"cell_type":"code","source":"\nclass CharBoundaryTagger(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.rnn = nn.LSTM(emb_dim, hidden, num_layers=2, dropout=0.2, batch_first=True, bidirectional=True)\n        self.out = nn.Linear(hidden * 2, 1)\n    def forward(self, x, lengths):\n        emb = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        h, _ = self.rnn(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        logits = self.out(h).squeeze(-1)\n        return logits[:, :-1]\n\ndef load_bilstm_autoconfig(paths, device=\"cpu\"):\n    with open(paths[\"char2id\"], \"r\", encoding=\"utf8\") as f:\n        char2id = json.load(f)\n\n    sd = torch.load(paths[\"weights\"], map_location=\"cpu\")\n    emb_dim = sd[\"emb.weight\"].shape[1] \n    out_in = sd[\"out.weight\"].shape[1] \n    hidden = out_in // 2\n\n    model = CharBoundaryTagger(len(char2id), emb_dim=emb_dim, hidden=hidden)\n    model.load_state_dict(sd, strict=True)\n    model.to(device).eval()\n    print(f\"[BiLSTM] loaded emb_dim={emb_dim}, hidden={hidden}, vocab={len(char2id)}\")\n    return model, char2id\n\n@torch.no_grad()\ndef boundary_probs_for_string(s: str, model, char2id, device=\"cpu\"):\n    s = s.lower().replace(\"ё\", \"е\")\n    if len(s) <= 1:\n        return []\n    def _one_pass(seq: str):\n        x = torch.tensor([[char2id.get(ch, 1) for ch in seq]], dtype=torch.long, device=device)\n        lengths = torch.tensor([x.size(1)], dtype=torch.long, device=device)\n        logits = model(x, lengths) # [1, T-1]\n        return torch.sigmoid(logits).squeeze(0).tolist()\n    if len(s) <= MAX_LEN:\n        return _one_pass(s)\n    win, step = MAX_LEN, MAX_LEN - 16\n    acc = [0.0]*(len(s)-1); cnt = [0]*(len(s)-1)\n    start = 0\n    while start < len(s):\n        end = min(len(s), start+win)\n        probs = _one_pass(s[start:end])\n        for k,p in enumerate(probs):\n            gi = start + k\n            if gi < len(acc):\n                acc[gi]+=p; cnt[gi]+=1\n        if end == len(s): break\n        start += step\n    return [acc[i]/max(1,cnt[i]) for i in range(len(acc))]\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBILSTM_PATHS = {\n    \"weights\": \"bilstm_ru_stream_v2.pth\",\n    \"char2id\": \"char2id_stream_v2.json\"  \n}\n\nmodel_bi, char2id_bi = load_bilstm_autoconfig(BILSTM_PATHS, device=DEVICE)\nprint(\"LSTM loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T20:22:44.515557Z","iopub.execute_input":"2025-09-24T20:22:44.515780Z","iopub.status.idle":"2025-09-24T20:22:44.565403Z","shell.execute_reply.started":"2025-09-24T20:22:44.515740Z","shell.execute_reply":"2025-09-24T20:22:44.564782Z"}},"outputs":[{"name":"stdout","text":"[BiLSTM] loaded emb_dim=128, hidden=256, vocab=1017\nLSTM loaded\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Собственно, основная функция сегментации restore_spaces_dp_lstm","metadata":{}},{"cell_type":"code","source":"def restore_spaces_dp_lstm(\n    text: str,\n    model=None, char2id=None, device=\"cpu\",\n    alpha: float = 0.65, \n    max_word_len: int = 28,\n    hint_bonus: float = 0.15,\n    use_logit: bool = True,\n    temp: float = 1.3,   \n    bias: float = -0.15, \n    conf_threshold: float = 0.55,\n    lambda_cut: float = 0.07,    \n    short_penalty: float = 0.8  \n) -> str:\n    if not isinstance(text, str) or len(text) == 0:\n        return \"\"\n\n    parts, i = [], 0\n    while i < len(text):\n        ch = text[i]\n        if ch in _PUNCT:\n            parts.append((ch, True))\n            i += 1\n        else:\n            j = i + 1\n            while j < len(text) and text[j] not in _PUNCT:\n                j += 1\n            parts.append((text[i:j], False))\n            i = j\n\n    tokens = []\n    for chunk, is_punct in parts:\n        if is_punct:\n            tokens.append(chunk)\n            continue\n\n        run = chunk\n        n = len(run)\n        if n == 0:\n            continue\n\n        hints = [False] * (n + 1)\n        for k in range(1, n):\n            if _hard_boundary(run[k - 1], run[k]):\n                hints[k] = True\n\n        probs = None\n        if (model is not None) and (char2id is not None) and (n > 1):\n            probs = boundary_probs_for_string(run, model, char2id, device=device)\n\n        dp = np.full(n + 1, -1e18, dtype=float)\n        prv = np.full(n + 1, -1, dtype=int)\n        dp[0] = 0.0\n\n        for end in range(1, n + 1):\n            start_lim = max(0, end - max_word_len)\n            best_val, best_j = -1e18, -1\n\n            for j in range(start_lim, end):\n                cand = run[j:end]\n                s = _cached_score_word(cand)\n\n                if len(cand) == 1 and cand.lower() not in _SHORT_OK:\n                    s -= short_penalty\n\n                if j > 0 and hints[j]:\n                    s += hint_bonus\n\n                if (probs is not None) and (j > 0) and (j < n):\n                    p = min(max(probs[j - 1], 1e-6), 1 - 1e-6)\n                    if p >= conf_threshold:\n                        if use_logit:\n                            logit = math.log(p) - math.log(1 - p)\n                            nn_term = (logit + bias) / max(1e-6, temp)\n                        else:\n                            logit = math.log(p) - math.log(1 - p)\n                            p_adj = 1 / (1 + math.exp(-(logit + bias) / max(1e-6, temp)))\n                            nn_term = math.log(p_adj)\n                        s += alpha * nn_term\n\n                if j > 0:\n                    s -= lambda_cut\n\n                val = dp[j] + s\n                if (val > best_val) or (val == best_val and j == start_lim):\n                    best_val, best_j = val, j\n\n            dp[end], prv[end] = best_val, best_j\n\n        if prv[n] == -1:\n            tokens.append(run)\n        else:\n            rev = []\n            k = n\n            while k > 0:\n                j = prv[k]\n                rev.append(run[j:k])\n                k = j\n            tokens.extend(rev[::-1])\n\n    out = []\n    for k, tok in enumerate(tokens):\n        out.append(tok)\n        if k == len(tokens) - 1:\n            break\n        nxt = tokens[k + 1]\n        if tok in _PUNCT:\n            if tok in {\",\", \"; \", \":\", \"!\", \"?\", \"…\", \".\"}:\n                out.append(\" \")\n            else:\n                out.append(\" \")\n        elif nxt in _PUNCT:\n            pass\n        else:\n            out.append(\" \")\n\n    result = \"\".join(out).strip()\n    if not isinstance(result, str):\n        result = str(result)\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T20:22:44.566129Z","iopub.execute_input":"2025-09-24T20:22:44.566345Z","iopub.status.idle":"2025-09-24T20:22:44.581630Z","shell.execute_reply.started":"2025-09-24T20:22:44.566322Z","shell.execute_reply":"2025-09-24T20:22:44.580908Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# ВЕС (alpha) вклада LSTM в итоговую оценку слова в DP. \n# Чем выше, тем больше доверия модели LSTM при принятии решения о разбиении.\nALPHA = 0.65\n\n# МАКСИМАЛЬНАЯ длина потенциального \"слова\", которое алгоритм будет пытаться \n# рассмотреть при сегментации (в DP). Слова длиннее будут отброшены на этапе перебора.\nMAX_WORD_LEN = 28\n\n# БОНУС, который добавляется к оценке слова, если оно начинается \n# в позиции, помеченной эвристикой `_hard_boundary` (например, смена регистра).\nHINT_BONUS = 0.15\n\n# ФЛАГ: использовать ли логит (разность логарифмов вероятностей) от LSTM \n# или напрямую вероятность пробела при расчёте вклада нейросети.\nUSE_LOGIT = True\n\n# ТЕМПЕРАТУРА для \"разогрева\" (или охлаждения) логита от LSTM перед тем, \n# как он пойдёт в оценку. Меньшее значение -> мягче вклад модели.\nTEMP = 1.3\n\n# СМЕЩЕНИЕ (bias) для логита от LSTM перед тем, как он пойдёт в оценку. \n# Положительное смещение -> сеть \"увереннее\" в пробеле, отрицательное - наоборот.\nBIAS = -0.15\n\n# ПОРОГ уверенности (вероятности пробела от LSTM), выше которого \n# её вклад вообще учитывается в оценке слова.\nCONF_TH = 0.55\n\n# ШТРАФ за разрез (то есть за установку пробела). \n# Уменьшение этого значения делает разбиение \"щедрее\".\nLAMBDA_CUT = 0.07\n\n# ШТРАФ за односимвольные слова (если они не входят в `_SHORT_OK`).\n# Уменьшение делает такие слова \"дешевле\", то есть алгоритм охотнее их разрешает.\nSHORT_PEN = 0.8\n\n# Устройство (GPU или CPU), на котором будет выполняться инференс модели.\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{}},{"cell_type":"code","source":"ALPHA = 0.65 # вклад LSTM в оценку слова в DP\nMAX_WORD_LEN = 28\nHINT_BONUS = 0.15 # эвристический бонус  \nUSE_LOGIT = True \nTEMP = 1.3 \nBIAS = -0.15 \nCONF_TH = 0.55 # порог уверенности LSTM (при уверенности ниже порога вклад LSTM не учитываем в DP)\nLAMBDA_CUT = 0.07 # штраф пробела \nSHORT_PEN = 0.8 # штраф за односимвольные слова\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npred_texts = []\nfor s in tqdm(task_data[\"text_no_spaces\"].tolist(), desc=\"Predicting DP+LSTM\"):\n    restored = restore_spaces_dp_lstm(\n        s,\n        model=model_bi, char2id=char2id_bi, device=DEVICE,\n        alpha=ALPHA, max_word_len=MAX_WORD_LEN, hint_bonus=HINT_BONUS,\n        use_logit=USE_LOGIT, temp=TEMP, bias=BIAS, conf_threshold=CONF_TH,\n        lambda_cut=LAMBDA_CUT, short_penalty=SHORT_PEN\n    )\n    pred_texts.append(restored)\n\ntask_data[\"restored_text\"] = pred_texts\nout_csv = \"predicted_text.csv\"\ntask_data[[\"id\", \"restored_text\"]].to_csv(out_csv, index=False, encoding=\"utf-8\")\nprint(\"Saved:\", out_csv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T20:28:55.749695Z","iopub.execute_input":"2025-09-24T20:28:55.750525Z","iopub.status.idle":"2025-09-24T20:28:58.484514Z","shell.execute_reply.started":"2025-09-24T20:28:55.750497Z","shell.execute_reply":"2025-09-24T20:28:58.483595Z"}},"outputs":[{"name":"stderr","text":"Predicting DP+LSTM: 100%|██████████| 1005/1005 [00:02<00:00, 369.47it/s]","output_type":"stream"},{"name":"stdout","text":"Saved: predicted_text.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def space_positions_compressed(restored_text: str) -> list[int]:\n    positions = []\n    cursor = 0\n    for ch in restored_text:\n        if ch == ' ':\n            positions.append(cursor)\n        else:\n            cursor += 1\n    return positions\n\npredicted_positions_list = []\nfor _, row in tqdm(task_data.iterrows(), total=len(task_data), desc=\"Finding space positions\"):\n    restored = row[\"restored_text\"]\n    positions = space_positions_compressed(restored)\n    predicted_positions_list.append(positions)\n\ntask_data[\"predicted_positions\"] = [str(xs) for xs in predicted_positions_list]\n\ndef create_submission_file(task_data, output_file: str):\n    sub = task_data[[\"id\", \"predicted_positions\"]].copy()\n    sub.to_csv(output_file, index=False, encoding=\"utf-8\")\n    print(f\"Submission file saved to {output_file}\")\n    print(sub.head())\n\ncreate_submission_file(task_data, \"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T20:28:58.485872Z","iopub.execute_input":"2025-09-24T20:28:58.486126Z","iopub.status.idle":"2025-09-24T20:28:58.550494Z","shell.execute_reply.started":"2025-09-24T20:28:58.486106Z","shell.execute_reply":"2025-09-24T20:28:58.549558Z"}},"outputs":[{"name":"stderr","text":"Finding space positions: 100%|██████████| 1005/1005 [00:00<00:00, 20689.59it/s]","output_type":"stream"},{"name":"stdout","text":"Submission file saved to submission.csv\n  id     predicted_positions\n0  0             [5, 10, 12]\n1  1           [1, 6, 7, 13]\n2  2  [4, 5, 12, 13, 20, 21]\n3  3             [5, 10, 18]\n4  4                 [5, 10]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16}]}